---
title: "Classical music and Movie Soundtracks"
author: "Emma Bakker"
date: "2023-03-13"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    theme: journal
    self_contained: false
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
library(flexdashboard)
library(spotifyr)
library(tidyverse)
library(ggplot2)
library(plotly)
library(tidyr)
library(dplyr)
library(compmus)
library(tidymodels)
library(ggdendro)
library(heatmaply)

Sys.setenv(SPOTIFY_CLIENT_ID = '76f37f6c523d482a8c825d01db224388')
Sys.setenv(SPOTIFY_CLIENT_SECRET = '934add8cf3744afc89d1391370327a5d')

access_token <- get_spotify_access_token()
 

classical_essentials <- get_playlist_audio_features("", "37i9dQZF1DWWEJlAGA9gs0")
iconic_soundtracks <- get_playlist_audio_features("", "37i9dQZF1DX1tz6EDao8it") 
```

Visual Analysis {.storyboard data-icon="fa-signal"}
======================================================

### Notable patters in Iconic Soundtracks found by hierarchical clustering

```{r}

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  

soundtracks <-
  get_playlist_audio_features("", "37i9dQZF1DX1tz6EDao8it") |>
  slice(1:20) |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

soundtracks_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = soundtracks
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(soundtracks |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")


soundtracks_dist <- dist(soundtracks_juice, method = "euclidean")

soundtracks_dist |> 
  hclust(method = "complete") |> # Try single, average, and complete.
  dendro_data() |>
  ggdendrogram()

heatmaply(
  soundtracks_juice,
  hclustfun = hclust,
  hclust_method = "average",  # Change for single, average, or complete linkage.
  dist_method = "euclidean",
  main = "Heatmap of Audio Features for 20 songs of Iconice Soundtracks playlist"
)

```

---

Here a heatmap and dendrogram based on the audio features of 20 songs from the "Classical Essentials" playlist are visualized. The heatmap displays the audio features of 20 songs from the Iconic Soundtracks playlist, where each row represents a song, and each column represents an audio feature. The colors in the heatmap correspond to the values of each audio feature, where yellow indicates higher values and blue indicates lower values.

The dendrogram illustrates the results of hierarchical clustering of the songs based on their audio features. The height of the dendrogram branches indicates the degree of similarity between songs.

Notable patterns found in the dendrogram show that the 20 songs can be clustered into two main groups. The first group consists of seven songs that are closely related to each other, forming a cluster on the right side of the dendrogram. The second group consists of 13 songs, which are further divided into two subclusters on the left side of the dendrogram.

Among the audio features, loudness seems to be the most useful feature for clustering, as songs with similar loudness values are clustered together. On the other hand, features such as valence and speechiness appear to be less useful for clustering, as there are no clear patterns of similarity or dissimilarity based on these features.

### Notable patters in Classical Essentials found by hierarchical clustering

```{r}

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
} 

classicals <-
  get_playlist_audio_features("", "37i9dQZF1DWWEJlAGA9gs0") |>
  slice(1:20) |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

classicals_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = classicals
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(classicals |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")


classicals_dist <- dist(classicals_juice, method = "euclidean")

classicals_dist |> 
  hclust(method = "complete") |> # Try single, average, and complete.
  dendro_data() |>
  ggdendrogram()

heatmaply(
  classicals_juice,
  hclustfun = hclust,
  hclust_method = "average",  # Change for single, average, or complete linkage.
  dist_method = "euclidean",
  main = "Heatmap of Audio Features for 20 songs of Classical Essentails"
)


```

---

Here a heatmap and dendrogram based on the audio features of 20 songs from the "Classical Essentials" playlist are visualized. The heatmap displays a color-coded grid where rows represent the songs and columns represent the audio features, such as danceability, energy, and tempo. Warmer colors indicate higher values while cooler colors represent lower values. The dendrogram is a tree-like diagram that shows the hierarchical relationships among the songs based on their audio feature similarities.

Looking at the dendrogram, we can observe that the songs form several distinct clusters, with some clusters being more closely related to each other than others. One notable pattern is that songs with similar tempos tend to cluster together. Additionally, songs with similar pitches also tend to group together. In contrast, the features related to the timbre of the music do not seem to have a strong effect on the clustering.

Regarding the heatmap, we can see that some features, such as loudness, energy, and danceability, show a relatively high degree of variability across the songs, while others, such as speechiness and instrumentalness, show little variability. Interestingly, the heatmap also shows that some songs have a distinctive profile of high or low values across multiple features, indicating that some songs share specific musical characteristics.

Overall, the dendrogram and heatmap provide a useful visualization of the relationships among the songs based on their audio features. It suggests that some features, such as tempo and pitch, are more useful than others for clustering classical music songs based on their audio features.

### What are the differences in tempograms between an outlier and a song that is representative of the rest of the playlist

```{r}
library(gridExtra)

not_classical <- get_tidy_audio_analysis("4fDodwdZ1LBDL2I2WCnUMY")
outlier_classical <- get_tidy_audio_analysis("1CDEWKmyRTPWWa7uIKVSU4")

not_soundtracks <- get_tidy_audio_analysis("1CxT3WZkSuEAxPOKAPzDDl")
outlier_soudtracks <- get_tidy_audio_analysis("5FRGTGPZ7uA6JaLAH55PVV")


p1 <- not_classical |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "'Tsar Saltan, Op. 57: Flight of the Bumblebee' by André Previn") +
  theme_classic()

p2 <- outlier_classical |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "'Job, a Masque for Dancing' by Ralph  Vaughan Williams") +
  theme_classic()

p3 <- not_soundtracks |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "'End Titles' by Rachel Portman") +
  theme_classic()

p4 <- outlier_soudtracks |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "'Brooks was here' by Thomas Newman") +
  theme_classic()

# Create plotly objects from the ggplot objects
p1_ly <- ggplotly(p1)
p2_ly <- ggplotly(p2)
p3_ly <- ggplotly(p3)
p4_ly <- ggplotly(p4)

# Arrange the plots on a grid
grid <- subplot(p1_ly, p2_ly, p3_ly, p4_ly, nrows = 2, margin = 0.05)

# Display the grid
grid

```

---

These four plots represent two different playlists: Iconic Soundtracks and Classical Essentials. The tempograms are a way to visualize the tempo of a piece of music over time. 

The first plot shows the tempo of 'Job, a Masque for Dancing' by Ralph Vaughan Williams, an outlier from the Classical Essentials playlist. The second plot shows the tempo of 'Tsar Saltan, Op. 57: Flight of the Bumblebee' by André Previn, a piece from the Classical Essentials playlist. Comparing the two Classical Essentials plots, we can see that 'Job, a Masque for Dancing' has a much more variable tempo than 'Tsar Saltan, Op. 57: Flight of the Bumblebee', with many more peaks and valleys in the tempo curve. This suggests that 'Job, a Masque for Dancing' has a more complex rhythmic structure than 'Tsar Saltan, Op. 57: Flight of the Bumblebee'.

The third plot shows the tempo of 'End Titles' by Rachel Portman, a representative example of the rest of the Iconic Soundtracks playlist. The fourth plot shows the tempo of 'Brooks was here' by Thomas Newman, an outlier from the Iconic Soundtracks playlist.'End Titles' has a relatively stable and consistent tempo throughout the entire track. On the other hand, 'Brooks was here', the outlier track, has a more variable tempo, with frequent changes in tempo throughout the track. Additionally, the tempo of 'Brooks was here' generally tends to be slower than the tempo of 'End Titles'.

Comparing 'End Titles' from the Iconic Soundtracks playlist to 'Tsar Saltan, Op. 57: Flight of the Bumblebee' from the Classical Essentials playlist, we can see that 'End Titles' has a more consistent and stable tempo than 'Tsar Saltan, Op. 57: Flight of the Bumblebee', which has more fluctuations in tempo. This difference may be due to the fact that the Iconic Soundtracks playlist includes music from a wider range of genres and styles than the Classical Essentials playlist, which may have a more narrow focus on classical music.

### What are the differences in keygrams between an outlier and a song that is representative of the rest of the playlist

```{r, figures-side, fig.show="hold", out.width="50%"}



par(mar = c(4, 4, .1, .1))

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

brooks_was_here <-
  get_tidy_audio_analysis("5FRGTGPZ7uA6JaLAH55PVV") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

brooks_was_here |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "'Brooks was here' by Thomas Newman", x = "Time (s)", y = "")


end_titles <-
  get_tidy_audio_analysis("1CxT3WZkSuEAxPOKAPzDDl") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

end_titles |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "'End Titles' by Rachel Portman", x = "Time (s)", y = "")


par(mar = c(4, 4, .1, .1))
 
vaughan <-
  get_tidy_audio_analysis("1CDEWKmyRTPWWa7uIKVSU4") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

vaughan |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "'Job, a Masque for Dancing' by Ralph  Vaughan Williams", x = "Time (s)", y = "")


tsar_saltan <-
  get_tidy_audio_analysis("4fDodwdZ1LBDL2I2WCnUMY") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

tsar_saltan |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "'Tsar Saltan, Op. 57: Flight of the Bumblebee' by André Previn", x = "Time (s)", y = "")



```

---

The top two chordograms are two songs from the Iconic Soundtracks playlist, namely "Brooks was here" by Thomas Newman and "End Titles" by Rachel Portman. The chordograms represent the chords used in the songs over time, with the x-axis representing time in seconds and the y-axis representing the different chords. The chords are represented using 1-0 coding for chord templates and the Krumhansl-Kessler key profiles.
The two chordograms show that the two songs have different chord progressions. "Brooks was here" has more varied and complex chords compared to "End Titles", which has a simpler chord progression. The chordogram for "Brooks was here" has more vertical lines, indicating more chord changes, whereas the chordogram for "End Titles" has more horizontal lines, indicating longer sections with the same chord.
Overall, the chordograms provide a visual representation of the harmonic structure of the two songs, highlighting differences in chord progression and complexity.
It is worth noting that "Brooks was here" stands out as an outlier of the Classical Essentials playlist, with a significantly different chord progression compared to the rest of the playlist. "End Titles" serves as a representative example of the typical chord progression in the playlist.

The two chordograms on the bottom are from two songs, 'Tsar Saltan, Op. 57: Flight of the Bumblebee' by André Previn, and 'Job, a Masque for Dancing' by Ralph Vaughan Williams. The chordograms are visual representations of the harmonic structure of the songs, with again time on the x-axis and the chords on the y-axis. The colors of the rectangles represent the strength of each chord based on the Krumhansl-Kessler key profile.
The two chordograms are significantly different. The chordogram of 'Tsar Saltan, Op. 57: Flight of the Bumblebee' has more consistent coloring and clear harmonic patterns. On the other hand, the chordogram of 'Job, a Masque for Dancing' is more complex, with a wider range of colors and less clear harmonic patterns. This difference is due to the fact that 'Tsar Saltan, Op. 57: Flight of the Bumblebee' is a piece from the Classical Essentials playlist and follows traditional harmonic patterns, while 'Job, a Masque for Dancing' is an outlier from the playlist and has more complex and unpredictable harmonic patterns.  

### What are the differences in audio features of a playlist with movie soundtracks of classical songs

```{r}
classical <- get_playlist_audio_features("", "37i9dQZF1DWWEJlAGA9gs0")
soundtracks <- get_playlist_audio_features("", "37i9dQZF1DX1tz6EDao8it")

# add playlist names to the data frames
classical$playlist <- "Classical Essentials"
soundtracks$playlist <- "Iconic Soundtracks"

# combine the playlists into one dataframe
audio_features <- bind_rows(classical, soundtracks)


# create a more colorful color scale based on acousticness
color_scale <- scale_color_gradient(low = "#FEE08B", high = "#D53E4F", guide = guide_colorbar(title = "Acousticness"))

# create scatterplot with geom_jitter() and geom_smooth()
scatter_plot <- ggplot(audio_features, aes(x = energy, y = loudness, color = acousticness, text = track.name)) +
  geom_jitter(alpha = 0.8, size = 2) +
  facet_wrap(~ playlist, scales = "free", labeller = labeller(playlist = c("Classical Essentials" = "Classical Essentials", "Iconic Soundtracks" = "Iconic Soundtracks"))) +
  geom_smooth(aes(group = playlist), method = "lm", se = FALSE, color = "black", alpha = 0.5, size = 1) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-45, -5), expand = c(0, 0)) +
  color_scale +
  labs(x = "Energy", y = "Loudness", title = "Scatterplot of Energy vs. Loudness for Classical and Soundtrack Playlists", color = "Acousticness") +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "gray90"), panel.grid.minor = element_blank(), panel.border = element_blank(), axis.line = element_line(colour = "black"), strip.text = element_text(size = 14)) 

# convert scatterplot to plotly object for interactivity
scatter_plot <- ggplotly(scatter_plot, tooltip = c("color", "text", "x", "y"), width = 1000, height = 500)

# set plot background to grid
scatter_plot %>% layout(plot_bgcolor = "white", paper_bgcolor = "white", xaxis = list(showgrid = TRUE, gridcolor = "gray90"), yaxis = list(showgrid = TRUE, gridcolor = "gray90"))

# display the plot
scatter_plot

```

---

This graph shows a scatterplot of the energy vs. loudness for two different playlists: "Classical Essentials" and "Iconic Soundtracks". Each point in the scatterplot represents a track, with the color of the point indicating the acousticness of the track. The plot also includes a linear regression line for each playlist, with no standard error shown.

In general, the "Iconic Soundtracks" playlist tends to have tracks with higher energy and higher loudness compared to the "Classical Essentials" playlist, which tends to have tracks with lower energy and lower loudness. However, there is some overlap between the playlists, particularly in the range of energy values around 0.4 and loudness values around -10.

The acousticness of the tracks in each playlist varies widely, with some tracks having very low acousticness values (indicating that they have a more electronic or synthetic sound) and others having very high acousticness values (indicating that they have a more natural or acoustic sound). The color scale used in the plot ranges from yellow (low acousticness) to red (high acousticness).

Overall, this plot provides a useful visual representation of the differences in energy and loudness between these two playlists and how the acousticness of the tracks varies within each playlist.

### Chroma and Timbre values of The Blade Runnner Blues

```{r}

# extract chroma values
get_chroma <- function(track.id) {
  audio_analysis <- get_track_audio_analysis(track.id)
  chroma <- audio_analysis$segments %>%
    select(start, pitches) %>%
    unnest(pitches) %>%
    mutate(pitch_class = rep(c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"), length.out = length(pitches)),
           intensity = as.numeric(pitches)) %>%
    select(-pitches) %>%
    spread(pitch_class, intensity, fill = 0)
  
  return(chroma)
}

get_timbre <- function(track.id) {
  audio_analysis <- get_track_audio_analysis(track.id)
  timbre <- audio_analysis$segments %>%
    select(start, timbre) %>%
    unnest(timbre) %>%
    mutate(timbre_number = rep(1:12, length.out = length(timbre)),
           value = as.numeric(timbre)) %>%
    select(-timbre) %>%
    spread(timbre_number, value, fill = 0)
  
  return(timbre)
}


# get chroma data frames for all tracks in iconic_soundtracks playlist
iconic_soundtracks_chroma <- map(iconic_soundtracks$track.id, get_chroma)

iconic_soundtracks_chroma <- map2(iconic_soundtracks_chroma, iconic_soundtracks$track.id, ~ .x %>% mutate(track_id = .y))

iconic_soundtracks_chroma <- bind_rows(iconic_soundtracks_chroma)

classical_essentials_chroma <- map(classical_essentials$track.id, get_chroma)

classical_essentials_chroma <- map2(classical_essentials_chroma, classical_essentials$track.id, ~ .x %>% mutate(track_id = .y))

classical_essentials_chroma <- bind_rows(classical_essentials_chroma)

# get chroma data frames for all tracks in iconic_soundtracks playlist
iconic_soundtracks_timbre <- map(iconic_soundtracks$track.id, get_timbre)

iconic_soundtracks_timbre <- map2(iconic_soundtracks_timbre, iconic_soundtracks$track.id, ~ .x %>% mutate(track_id = .y))

iconic_soundtracks_timbre <- bind_rows(iconic_soundtracks_timbre)

classical_essentials_timbre <- map(classical_essentials$track.id, get_timbre)

classical_essentials_timbre <- map2(classical_essentials_timbre, classical_essentials$track.id, ~ .x %>% mutate(track_id = .y))

classical_essentials_timbre <- bind_rows(classical_essentials_timbre)


# get chroma and timbre data for the outlier track
outlier_chroma <- get_chroma("575blCgesVtCu0HEYaIcas")
outlier_timbre <- get_timbre("575blCgesVtCu0HEYaIcas")

# create chromagram plot
chromagram <- outlier_chroma %>%
  select(-start) %>%
  as.matrix() %>%
  t() %>%
  round(3) %>%
  plot_ly(
    x = 1:12, y = 0:20, z = .,
    type = "heatmap", colorscale = "Viridis"
  ) %>%
  layout(
    title = "Chromagram for Blade Runner Blues",
    xaxis = list(title = "<b>Pitch Class</b>", tickvals = 1:12, ticktext = c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"), showgrid = FALSE),
    yaxis = list(title = "<b>Time (seconds)</b>", showgrid = FALSE),
    hoverlabel = list(
      bgcolor = "white",
      font = list(size = 12),
      bordercolor = "black"
    )
  )

# create cepstrogram plot
cepstrogram <- outlier_timbre %>%
  select(-start) %>%
  as.matrix() %>%
  t() %>%
  round(3) %>%
  plot_ly(
    x = 1:12, y = 0:20, z = .,
    type = "heatmap", colorscale = "ViridisLite"
  ) %>%
  layout(
    title = "Cepstrogram for Blade Runner Blues",
    xaxis = list(title = "<b>Cepstral Coefficient</b>", showgrid = FALSE),
    yaxis = list(title = "<b>Time (seconds)</b>", showgrid = FALSE),
    hoverlabel = list(
      bgcolor = "white",
      font = list(size = 12),
      bordercolor = "black"
    )
  )


# create combined plot
combined_plot <- subplot(
  chromagram, cepstrogram,
  nrows = 2, margin = 0.05,
  heights = c(0.5, 0.5),
  titleX = FALSE, titleY = FALSE
)

# add description
description <- description <- "<b>Chromagram:</b> The distribution of the 12 pitch classes over time in the selected song.<br><b>Cepstrogram:</b> The distribution of the 12 cepstral coefficients over time, which are related to the timbre of the sound. The combined plot allows for a comparison of the two features, highlighting the rhythmic and tonal characteristics of the song. When hovering over the chromagram plot or cepstrogram plot, the x values show the pitch class or cepstral coefficient, the y value represents the time in seconds and the z value represents the intensity of the pitch class or cepstral coefficient at the given time."

combined_plot <- combined_plot %>% layout(
  title = "Chromagram and Cepstrogram for Blade Runner Blues",
  annotations = list(
    x = 0.5, y = -0.12,
    text = description,
    showarrow = FALSE,
    xref = "paper",
    yref = "paper",
    font = list(size = 12)
  )
)

# display plot
combined_plot

```

---

<b>Chromagram:</b> The distribution of the 12 pitch classes over time in the selected song.<br><b>Cepstrogram:</b> The distribution of the 12 cepstral coefficients over time, which are related to the timbre of the sound. The combined plot allows for a comparison of the two features, highlighting the rhythmic and tonal characteristics of the song. When hovering over the chromagram plot or cepstrogram plot, the x values show the pitch class or cepstral coefficient, the y value represents the time in seconds and the z value represents the intensity of the pitch class or cepstral coefficient at the given time.

Looking at the cepstrogram plot, we can see that the song "Blade Runner Blues" has a relatively smooth and uniform distribution of timbre features over time, with a prominent peak at the lower cepstral coefficients. This suggests a relatively low level of roughness in the sound, which is consistent with the overall mellow and atmospheric mood of the song.

In terms of brightness and warmth, the cepstrogram does not reveal any particularly strong or distinctive patterns, which could suggest that these timbral features are not as salient in this song as other characteristics such as the use of ambient textures and electronic instrumentation.

Given that "Blade Runner Blues" is part of a playlist of iconic soundtracks, it is worth noting that the song was composed by Vangelis for the soundtrack of the 1982 film "Blade Runner", which is considered a landmark of science fiction cinema. The use of electronic instruments and atmospheric textures in the song is consistent with the film's dystopian and futuristic themes, while the melancholic and introspective mood of the music reflects the emotional depth and complexity of the film's characters and themes.

Overall, the chromagram and cepstrogram plots provide valuable insights into the melodic and timbral characteristics of "Blade Runner Blues", which can be used to shed light on the song's structure, style, and meaning, as well as its cultural and historical significance as part of the iconic "Blade Runner" soundtrack.


### Chroma Values Soundstrack outlier

```{r}

# get playlist data
iconic_soundtracks <- get_playlist_audio_features("","37i9dQZF1DX1tz6EDao8it")
classical_essentials <- get_playlist_audio_features("","37i9dQZF1DWWEJlAGA9gs0")

# extract chroma values
get_chroma <- function(track.id) {
  audio_analysis <- get_track_audio_analysis(track.id)
  chroma <- audio_analysis$segments %>%
    select(start, pitches) %>%
    unnest(pitches) %>%
    mutate(pitch_class = rep(c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"), length.out = length(pitches)),
           intensity = as.numeric(pitches)) %>%
    select(-pitches) %>%
    spread(pitch_class, intensity, fill = 0)
  
  return(chroma)
}


# get chroma data frames for all tracks in iconic_soundtracks playlist
iconic_soundtracks_chroma <- map(iconic_soundtracks$track.id, get_chroma)

iconic_soundtracks_chroma <- map2(iconic_soundtracks_chroma, iconic_soundtracks$track.id, ~ .x %>% mutate(track_id = .y))

iconic_soundtracks_chroma <- bind_rows(iconic_soundtracks_chroma)

chroma_data <- iconic_soundtracks_chroma %>%
  filter(track_id == "575blCgesVtCu0HEYaIcas")

chroma_data <- chroma_data %>%
  select(-track_id)

chroma_data <- chroma_data %>%
  gather(note, value, -start)

chromagram <- chroma_data %>%
  group_by(note) %>%
  summarise(chroma = sum(value))



plotly_chroma_feature <- chromagram %>%
  plot_ly(x = ~note, y = ~chroma, type = "bar",
          text = ~paste0(round(chroma * 100, 1), "%"),
          hoverinfo = "text",
          marker = list(color = "rgba(55, 128, 191, 0.7)",
                        line = list(color = "rgba(55, 128, 191, 1.0)",
                                    width = 1))) %>%
  layout(xaxis = list(title = "Note"),
         yaxis = list(title = "Chroma value"),
         title = list(text = "Chroma feature Blade Runner Blues"))

plotly_chroma_feature


```

---

These plots show the chroma features for 'Blade Runner Blues' from the "iconic_soundtracks" Spotify playlist and ... 

The chroma feature is a way of representing the tonal content of an audio signal. It is based on the 12 different pitches in a chromatic scale (C, C#, D, D#, E, F, F#, G, G#, A, A#, B), and for each pitch it calculates a value that represents the amount of energy in the audio signal that corresponds to that pitch.

In this plot, the x-axis shows the 12 different pitches (notes) in the chromatic scale, and the y-axis shows the corresponding chroma value for each note. The height of each bar represents the amount of energy in the audio signal that corresponds to that note, and the color of each bar indicates the magnitude of the chroma value (with darker colors indicating higher values). The hover text displays the exact percentage of energy in the audio signal that corresponds to each note.

In summary, this plot shows the relative distribution of energy across the 12 different pitches in the audio signal for a specific track.

### Chroma Values Classical outlier 

```{r}

# extract chroma values
get_chroma <- function(track.id) {
  audio_analysis <- get_track_audio_analysis(track.id)
  chroma <- audio_analysis$segments %>%
    select(start, pitches) %>%
    unnest(pitches) %>%
    mutate(pitch_class = rep(c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"), length.out = length(pitches)),
           intensity = as.numeric(pitches)) %>%
    select(-pitches) %>%
    spread(pitch_class, intensity, fill = 0)
  
  return(chroma)
}

# get chroma data frames for all tracks in iconic_soundtracks playlist

classical_essentials_chroma <- map(classical_essentials$track.id, get_chroma)

classical_essentials_chroma <- map2(classical_essentials_chroma, classical_essentials$track.id, ~ .x %>% mutate(track_id = .y))

classical_essentials_chroma <- bind_rows(classical_essentials_chroma)


chroma_data <- classical_essentials_chroma %>%
  filter(track_id == "5fdp9rXfEixCGLM1Og4EN1")

chroma_data <- chroma_data %>%
  select(-track_id)

chroma_data <- chroma_data %>%
  gather(note, value, -start)

chromagram <- chroma_data %>%
  group_by(note) %>%
  summarise(chroma = sum(value))


plotly_chroma_feature <- chromagram %>%
  plot_ly(x = ~note, y = ~chroma, type = "bar",
          text = ~paste0(round(chroma * 100, 1), "%"),
          hoverinfo = "text",
          marker = list(color = "rgba(55, 128, 191, 0.7)",
                        line = list(color = "rgba(55, 128, 191, 1.0)",
                                    width = 1))) %>%
  layout(xaxis = list(title = "Note"),
         yaxis = list(title = "Chroma value"),
         title = list(text = "Chroma feature for track Gnossienne No. 1"))

plotly_chroma_feature

```

---

These plots show the chroma features for 'Blade Runner Blues' from the "iconic_soundtracks" Spotify playlist and ... 

The chroma feature is a way of representing the tonal content of an audio signal. It is based on the 12 different pitches in a chromatic scale (C, C#, D, D#, E, F, F#, G, G#, A, A#, B), and for each pitch it calculates a value that represents the amount of energy in the audio signal that corresponds to that pitch.

In this plot, the x-axis shows the 12 different pitches (notes) in the chromatic scale, and the y-axis shows the corresponding chroma value for each note. The height of each bar represents the amount of energy in the audio signal that corresponds to that note, and the color of each bar indicates the magnitude of the chroma value (with darker colors indicating higher values). The hover text displays the exact percentage of energy in the audio signal that corresponds to each note.

In summary, this plot shows the relative distribution of energy across the 12 different pitches in the audio signal for a specific track.

 
Introduction {data-icon="fa-file-text"}
=======================================================

### Difference between classical music and movie soundtracks

For my portfolio, I have chosen a corpus that compares classical music with film soundtracks. The corpus will include track from the “Classical Essential” and “Iconic Soundtracks” playlists on Spotify. I am interested in this because both classical music and film soundtracks can evoke emotions in listeners, but the audiences of both can be very different, which is why I want to see what differentiates the two.

In my corpus I will compare classical compositions with film soundtracks. Within these groups, there are various natural subgroups such as individual composers, specific movies, and musical genres. I expect to find differences in instrumentation and production between the two groups, as well as differences between individual composers and genres. However, I also anticipate finding commonalities in melody, harmony, and musical structure.

The tracks in my corpus are representative of the groups I want to compare, but there are probably some recordings missing or variations that could affect the analysis. For example, there may be differences between the live and studio versions of a classical piece that are not reflected in the corpus. I will keep this in mind when the data.

One atypical track in my corpus is "Hedwig's Theme" by John Williams, which for most of the track uses only the celesta and a unique melody. A typical track is "Für Elise" by Ludwig van Beethoven, which is a well-known classical piece with a recognizable melody.
